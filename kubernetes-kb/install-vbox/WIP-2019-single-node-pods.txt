==========================================
SINGLE NODE CLUSTER WITH PODS
==========================================

https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4
https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/
apiserver as pod - https://github.com/kubernetes/kubeadm/issues/522

======
VMs
======
VM1
hostname = ks1
IP = 192.168.60.101

=========
ETCD
=========

----
File: /etc/environment
----
# ETCD RELATED ENVIRONMENT
REGISTRY=quay.io/coreos/etcd
# available from v3.2.5
#REGISTRY=gcr.io/etcd-development/etcd

# For each machine
#ETCD_VERSION=latest
ETCD_VERSION=v3.2.24
# Note: TOKEN can be any string
TOKEN=my-etcd-token
CLUSTER_STATE=new
CLUSTER="ks1=http://192.168.60.101:2380"
DATA_DIR=/var/lib/etcd
THIS_NAME=ks1
THIS_IP=192.168.60.101

----
File: /etc/systemd/system/etcd.service:
----
[Unit]
Description=etcd
Documentation=https://github.com/coreos
Wants=docker.service

[Service]
Type=simple
User=root
Group=root
IOSchedulingClass=2
IOSchedulingPriority=0
EnvironmentFile=/etc/environment

# START ETCD
ExecStart=/usr/bin/docker run --rm\
  --net=host \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --name etcd ${REGISTRY}:${ETCD_VERSION} \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name ${THIS_NAME} \
  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \
  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \
  --initial-cluster ${CLUSTER} \
  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target


- NOW, START ETCD SERVICE
-- Reload the daemon configuration.
systemctl daemon-reload

-- Enable etcd to start at boot time.
systemctl enable etcd

-- Start etcd.
systemctl start etcd

-- Verify
# ps -ef| grep etcd
--> THE DOCKER COMMAND, INVOKED BY THE SERVICE
root      7702     1  0 18:33 ?        00:00:00 /usr/bin/docker run --rm --net=host -p 2379:2379 -p 2380:2380 --volume=/var/lib/etcd:/etcd-data --name etcd quay.io/coreos/etcd:v3.2.24 /usr/local/bin/etcd --data-dir=/etcd-data --name ks1 --initial-advertise-peer-urls http://192.168.60.101:2380 --listen-peer-urls http://0.0.0.0:2380 --advertise-client-urls http://192.168.60.101:2379 --listen-client-urls http://0.0.0.0:2379 --initial-cluster ks1=http://192.168.60.101:2380 --initial-cluster-state new --initial-cluster-token my-etcd-token

--> THE ETCD EXECUTABLE INSIDE CONTAINER
root      7723  7708  0 18:33 ?        00:00:00 /usr/local/bin/etcd --data-dir=/etcd-data --name ks1 --initial-advertise-peer-urls http://192.168.60.101:2380 --listen-peer-urls http://0.0.0.0:2380 --advertise-client-urls http://192.168.60.101:2379 --listen-client-urls http://0.0.0.0:2379 --initial-cluster ks1=http://192.168.60.101:2380 --initial-cluster-state new --initial-cluster-token my-etcd-token

-- Verify cluster members
# docker exec -ti etcd /bin/sh
/ # (now, this is etcd container's shell)

/ # etcdctl member list
438ee572a442fb6c: name=ks1 peerURLs=http://192.168.60.101:2380 clientURLs=http://192.168.60.101:2379 isLeader=true


=================================
CREATE CONFIG AND MANIFEST FILES
=================================

------------------------------------------
/etc/kubernetes/kubelet-config.yaml:
------------------------------------------
(taken from https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4)

apiVersion: v1
clusters:
- cluster:
    server: http://127.0.0.1:8080
  name: kubernetes-systemd
contexts:
- context:
    cluster: kubernetes-systemd
    user: system:node:ks1
  name: system:node:ks1@kubernetes-systemd
current-context: system:node:ks1@kubernetes-systemd
kind: Config
preferences: {}
users:
- name: system:node:ks1

--- REFERENCE
apiVersion: v1
clusters:
- cluster:
    server: http://127.0.0.1:8080
  name: kubernetes-systemd
contexts:
- context:
    cluster: kubernetes-systemd
    user: system:node:<HOSTNAME>
  name: system:node:<HOSTNAME>@kubernetes-systemd
current-context: system:node:<HOSTNAME>@kubernetes-systemd
kind: Config
preferences: {}
users:
- name: system:node:<HOSTNAME>

------------------------------------------------------------------------------------
/etc/kubernetes/manifests/kube-apiserver.yaml:
------------------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.60.101
    - --insecure-bind-address=127.0.0.1
    - --bind-address=0.0.0.0
    - --etcd-servers=http://192.168.60.101:2379
    - --service-cluster-ip-range=10.96.0.0/12
    - --admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
    image: gcr.io/google_containers/kube-apiserver-amd64:v1.12.8
    name: kube-apiserver

--- REFERENCE
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - command:
    - kube-apiserver
    - --insecure-bind-address=127.0.0.1
    - --etcd-servers=http://127.0.0.1:2379
    - --service-cluster-ip-range=10.96.0.0/12
    - --admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
    image: gcr.io/google_containers/kube-apiserver-amd64:v1.9.6
    name: kube-apiserver
    
------------------------------------------------------------------------------------
/etc/kubernetes/manifests/kube-controller-manager.yaml:
------------------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --cluster-name=kubernetes
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    - --cluster-cidr=10.20.0.0/16
    - --service-cluster-ip-range=10.96.0.0/12
    image: gcr.io/google_containers/kube-controller-manager-amd64:v1.12.8
    name: kube-controller-manager
  hostNetwork: true

--- REFERENCE 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --master=http://127.0.0.1:8080
    image: gcr.io/google_containers/kube-controller-manager-amd64:v1.9.6
    name: kube-controller-manager
  hostNetwork: true
  
------------------------------------------------------------------------------------
/etc/kubernetes/manifests/kube-scheduler.yaml:
------------------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    image: gcr.io/google_containers/kube-scheduler-amd64:v1.12.8
    name: kube-scheduler
  hostNetwork: true

--- REFERENCE
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --master=http://127.0.0.1:8080
    image: gcr.io/google_containers/kube-scheduler-amd64:v1.9.6
    name: kube-scheduler
  hostNetwork: true

----------------
Kubelet service
----------------
Notes kubelet.txt - read about kubeconfig and config parameters

NOTE:
Installing kubelet also will install a couple of service files:
/usr/lib/systemd/system/kubelet.service
--> which references indirectly this - /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

File: /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf:

# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
# -- commented
#Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
# -- commented
#Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
#
# -- added
Environment="KUBELET_CONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet-config.yaml --pod-manifest-path=/etc/kubernetes/manifests"

# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 

----------------
START KUBELET
----------------
# systemctl enable kubelet
# systemctl start kubelet

- VERIFY
# kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
kube-system   kube-apiserver-ks1            1/1     Running   0          8m33s
kube-system   kube-controller-manager-ks1   1/1     Running   0          8m57s
kube-system   kube-scheduler-ks1            1/1     Running   0          8m50s


===================================
CERTIFICATES
===================================
Note: The element 'ST' could be something like 'MyCompany Inc'

-----------------
CREATE CA CERT
-----------------

ca-config.json file:

{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}

ca-csr.json file:

{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "IN",
      "L": "INDIA",
      "O": "Kubernetes",
      "OU": "Kubernetes",
      "ST": "Kubernetes"
    }
  ]
}

Run command: (gen.sh)

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

Get these:

ca.csr - file containing the csr
ca.pem - ca public certificate
ca-key.pem - ca private key

------------------------
Admin client certificate
------------------------
This certificate will be used to connect to the Kubernetes cluster as an administrator.
NOTE:  No IP/hostname for this

admin-csr.json file:

{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "IN",
      "L": "INDIA",
      "O": "system:masters",
      "OU": "Kubernetes",
      "ST": "Kubernetes"
    }
  ]
}

Run command: (gen-admin.sh)

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes admin-csr.json | \
cfssljson -bare admin

Get these:

admin.csr
admin-key.pem
admin.pem

---------------------------
Kubelet client certificates (worker node)
---------------------------
Kubelets will need a certificate to join the Kubernetes cluster
NOTE: The IP is for the specific WORKER node

kubelet-192.168.60.102-csr.json file:

{
  "CN": "system:node:192.168.60.102",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "IN",
      "L": "INDIA",
      "O": "system:nodes",
      "OU": "Kubernetes",
      "ST": "Kubernetes"
    }
  ]
}

Run command : (gen-kubelet-192.168.60.102.sh)

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-hostname=192.168.60.102 \
-profile=kubernetes 192.168.60.102-csr.json | \
cfssljson -bare 192.168.60.102

Get these:

192.168.60.102.csr
192.168.60.102-key.pem
192.168.60.102.pem

--------------------------------------------
Generate the kube-proxy client certificate (worker nodes)
--------------------------------------------

kube-proxy-csr.json file:

{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "IN",
      "L": "INDIA",
      "O": "system:node-proxier",
      "OU": "Kubernetes",
      "ST": "Kubernetes"
    }
  ]
}

Run command: (kube-proxy-gen.sh)

cfssl gencert \
-ca=ca.pem -ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes kube-proxy-csr.json | \
cfssljson -bare kube-proxy

Get these:
kube-proxy.csr
kube-proxy-key.pem
kube-proxy.pem

--------------------------------------------
Generate the API server certificate
--------------------------------------------

kubernetes-csr.json file: 

{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "IN",
      "L": "INDIA",
      "O": "Kubernetes",
      "OU": "Kubernetes",
      "ST": "Kubernetes"
    }
  ]
}

Run command : (kubernetes-gen.sh)

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-hostname=192.168.60.102,127.0.0.1,kubernetes.default \
-profile=kubernetes kubernetes-csr.json | \
cfssljson -bare kubernetes

Get these:
kubernetes.pem
kubernetes-key.pem
kubernetes.csr

---------------------
COPY THE CERTIFICATES
---------------------
NOTE: As this is single node, all pem files are copied off to same directory
      In the case of master and worker being on different nodes copy different specific files to masters and workers

mkdir /etc/kubernetes/pki
chmod 755 /etc/kubernetes/pki

cp *pem /etc/kubernetes/pki
(master node part - ca.pem, ca-key.pem, kubernetes-key.pem kubernetes.pem )
(worker node part - ca.pem, 192.168.60.102-key.pem, 192.168.60.102.pem )


-------------------------------
/etc/environment - with https
-------------------------------
# ETCD RELATED ENVIRONMENT
REGISTRY=quay.io/coreos/etcd
# available from v3.2.5
#REGISTRY=gcr.io/etcd-development/etcd

# For each machine
#ETCD_VERSION=latest
ETCD_VERSION=v3.2.24
# Note: TOKEN can be any string
TOKEN=my-etcd-token
CLUSTER_STATE=new
CLUSTER="ks2=https://192.168.60.102:2380"
DATA_DIR=/var/lib/etcd
THIS_NAME=ks2
THIS_IP=192.168.60.102


-------------------------------
etcd.service - with certificates
-------------------------------

[Unit]
Description=etcd
Documentation=https://github.com/coreos
Wants=docker.service

[Service]
Type=simple
User=root
Group=root
IOSchedulingClass=2
IOSchedulingPriority=0
EnvironmentFile=/etc/environment

# START ETCD
ExecStart=/usr/bin/docker run --rm\
  --net=host \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --volume=/etc/kubernetes/pki:/etc/kubernetes/pki \
  --name etcd ${REGISTRY}:${ETCD_VERSION} \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name ${THIS_NAME} \
  --initial-advertise-peer-urls https://${THIS_IP}:2380 --listen-peer-urls https://0.0.0.0:2380 \
  --advertise-client-urls https://${THIS_IP}:2379 --listen-client-urls https://0.0.0.0:2379 \
  --initial-cluster ${CLUSTER} \
  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} \
  --cert-file=/etc/kubernetes/pki/kubernetes.pem \
  --key-file=/etc/kubernetes/pki/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/pki/ca.pem \
  --peer-cert-file=/etc/kubernetes/pki/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/pki/kubernetes-key.pem \
  --peer-trusted-ca-file=/etc/kubernetes/pki/ca.pem 

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target


- RESTART ETCD SERVICE (now with certificates)
systemctl daemon-reload
systemctl restart etcd

----------------------
kube-apiserver.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.60.102
    - --insecure-bind-address=127.0.0.1
    - --bind-address=0.0.0.0
    - --etcd-servers=https://192.168.60.102:2379
    - --service-cluster-ip-range=10.96.0.0/12
    - --admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
    - --etcd-cafile=/etc/kubernetes/pki/ca.pem 
    - --etcd-certfile=/etc/kubernetes/pki/kubernetes.pem 
    - --etcd-keyfile=/etc/kubernetes/pki/kubernetes-key.pem 
    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.pem 
    - --kubelet-client-certificate=/etc/kubernetes/pki/kubernetes.pem 
    - --kubelet-client-key=/etc/kubernetes/pki/kubernetes-key.pem 
    - --service-account-key-file=/etc/kubernetes/pki/ca-key.pem 
    - --tls-cert-file=/etc/kubernetes/pki/kubernetes.pem 
    - --tls-private-key-file=/etc/kubernetes/pki/kubernetes-key.pem
    - --secure-port=6443
    image: gcr.io/google_containers/kube-apiserver-amd64:v1.12.8
    name: kube-apiserver
    volumeMounts:
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki
      type: Directory
    name: k8s-certs

----------------------------
kube-controller-manager.yaml
----------------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - command:
    - kube-controller-manager
    - --cluster-name=kubernetes
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    - --cluster-cidr=10.20.0.0/16
    - --service-cluster-ip-range=10.96.0.0/12
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem 
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem
    - --root-ca-file=/etc/kubernetes/pki/ca.pem
    - --service-account-private-key-file=/etc/kubernetes/pki/ca-key.pem
    image: gcr.io/google_containers/kube-controller-manager-amd64:v1.12.8
    name: kube-controller-manager
    volumeMounts:
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki
      type: Directory
    name: k8s-certs

--------------------
kube-scheduler.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    image: gcr.io/google_containers/kube-scheduler-amd64:v1.12.8
    name: kube-scheduler
  hostNetwork: true

------------------
CREATE CLUSTER
------------------
systemctl start etcd
systemctl start kubelet

------------------
CREATE A POD
------------------
This is giving error now 

# kubectl run access --rm -ti --image busybox /bin/sh

kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.
If you don't see a command prompt, try pressing enter.
Error attaching, falling back to logs: error dialing backend: x509: certificate signed by unknown authority
deployment.apps "access" deleted
Error from server: Get https://ks2:10250/containerLogs/default/access-56ff88b445-gstms/access: x509: certificate signed by unknown authority



~~~~~~~~~~~~~~~~~~~~~~~~~~~
TROUBLESHOOTING
~~~~~~~~~~~~~~~~~~~~~~~~~~~

== TROUBLESHOOTING
All containers started, except kube-apiserver - which exited with the following error:

# docker logs d5fa03e15ed9
F0730 11:08:59.338023       1 storage_decorator.go:57] Unable to create storage backend: config (&{ /registry [https://192.168.60.101:2379]    true true 1000 0xc42013a360 <nil> 5m0s 1m0s}), err (context deadline exceeded)

https://github.com/kubernetes/kubernetes/issues/72102
https://stackoverflow.com/questions/50865788/kube-apiserver-unable-to-create-storage-backend
http://dockone.io/question/4034

- FIX
ETCD URL was incorrectly specified in kube-apiserver.yaml as https while cert-key security was not yet in place
Change that to http as follows:
- --etcd-servers=http://192.168.60.101:2379

== TROUBLESHOOTING 2
Kubelet not registering node:
...
...
Aug 07 13:13:43 ks2 kubelet[9938]: E0807 13:13:43.150737    9938 kubelet.go:2236] node "ks2" not found
Aug 07 13:13:43 ks2 kubelet[9938]: E0807 13:13:43.253045    9938 kubelet.go:2236] node "ks2" not found
Aug 07 13:13:43 ks2 kubelet[9938]: I0807 13:13:43.272527    9938 kubelet_node_status.go:73] Successfully registered node ks2
Aug 07 13:13:43 ks2 kubelet[9938]: I0807 13:13:43.279047    9938 reconciler.go:154] Reconciler: start to sync state
Aug 07 13:13:43 ks2 kubelet[9938]: E0807 13:13:43.351997    9938 kubelet_node_status.go:378] Error updating node status, will retry: error getting node "ks2": nodes "ks2" not found

- FIX
Restart kubelet seemed to fix it.

Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.341795   12155 volume_manager.go:248] Starting Kubelet Volume Manager
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.344390   12155 desired_state_of_world_populator.go:130] Desired state populator starts to run
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.442935   12155 kubelet.go:1821] skipping pod synchronization - [container runtime is down]
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.442978   12155 kubelet_node_status.go:277] Setting node annotation to enable volume controller attach/detach
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.624051   12155 kubelet_node_status.go:70] Attempting to register node ks2
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.651884   12155 kubelet_node_status.go:112] Node ks2 was previously registered
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.651906   12155 kubelet_node_status.go:73] Successfully registered node ks2
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.660025   12155 kubelet.go:1821] skipping pod synchronization - [container runtime is down]
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.803039   12155 setters.go:518] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2019-08-07 13:17:41.803019378 +0530 IST m=+1.104673113 LastTransitionTime:2019-08-07 13:17:41.803019378 +0530 IST m=+1.104673113 Reason:KubeletNotReady Message:container runtime is down}
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.973846   12155 cpu_manager.go:155] [cpumanager] starting with none policy
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.973868   12155 cpu_manager.go:156] [cpumanager] reconciling every 10s
Aug 07 13:17:41 ks2 kubelet[12155]: I0807 13:17:41.973875   12155 policy_none.go:42] [cpumanager] none policy: Start
Aug 07 13:17:42 ks2 kubelet[12155]: E0807 13:17:42.074896   12155 kubelet.go:1637] Failed creating a mirror pod for "kube-scheduler-ks2_kube-system(50c22c6953e39f9c091c1e41cf6efa3b)": pods "kube-scheduler-ks2" already exists
Aug 07 13:17:42 ks2 kubelet[12155]: E0807 13:17:42.080510   12155 kubelet.go:1637] Failed creating a mirror pod for "kube-controller-manager-ks2_kube-system(7b4edec443fb47a1d2aa7c66fa065598)": pods "kube-controller-manager-ks2" already exists
Aug 07 13:17:42 ks2 kubelet[12155]: E0807 13:17:42.080681   12155 kubelet.go:1637] Failed creating a mirror pod for "kube-apiserver-ks2_kube-system(a73022fd2d7a2a14ebc70b55736a8d4c)": pods "kube-apiserver-ks2" already exists
Aug 07 13:17:42 ks2 kubelet[12155]: I0807 13:17:42.211010   12155 reconciler.go:154] Reconciler: start to sync state

-- NOTES --
Installing kubelet also will install a couple of service files:
/usr/lib/systemd/system/kubelet.service
--> which references indirectly this - /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

- STARTING IT AS IS ERRORS OUT - modify the service file as above and restart kubelet
[root@ks1 kubelet.service.d]# systemctl enable kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.

[root@ks1 kubelet.service.d]# systemctl start kubelet

[root@ks1 kubelet.service.d]# journalctl -fu kubelet
-- Logs begin at Mon 2019-07-29 18:26:16 IST. --
Jul 29 19:39:27 ks1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Jul 29 19:39:27 ks1 kubelet[9896]: F0729 19:39:27.336390    9896 server.go:190] failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file "/var/lib/kubelet/config.yaml", error: open /var/lib/kubelet/config.yaml: no such file or directory
Jul 29 19:39:27 ks1 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a
Jul 29 19:39:27 ks1 systemd[1]: Unit kubelet.service entered failed state.
Jul 29 19:39:27 ks1 systemd[1]: kubelet.service failed.
Jul 29 19:39:37 ks1 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Jul 29 19:39:37 ks1 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Jul 29 19:39:37 ks1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Jul 29 19:39:37 ks1 kubelet[9909]: F0729 19:39:37.511865    9909 server.go:190] failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file "/var/lib/kubelet/config.yaml", error: open /var/lib/kubelet/config.yaml: no such file or directory
Jul 29 19:39:37 ks1 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a
Jul 29 19:39:37 ks1 systemd[1]: Unit kubelet.service entered failed state.
Jul 29 19:39:37 ks1 systemd[1]: kubelet.service failed.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
EXAMPLES, REFERENCES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Following are some example possibilities:
/etc/systemd/system/kubelet.service:
https://github.com/kubernetes-retired/contrib/blob/master/init/systemd/kubelet.service
(reference from https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4)

[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://kubernetes.io/docs/concepts/overview/components/#kubelet https://kubernetes.io/docs/reference/generated/kubelet/
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBELET_KUBECONFIG \
	    $KUBELET_ADDRESS \
	    $KUBELET_PORT \
	    $KUBELET_HOSTNAME \
	    $KUBE_ALLOW_PRIV \
	    $KUBELET_ARGS
Restart=on-failure
KillMode=process

[Install]
WantedBy=multi-user.target

~~~~
PROD OPTION:
~~~~
/etc/systemd/system/kubelet.service:

[Unit]
Description=kubelet: The Kubernetes Node Agent
Documentation=http://kubernetes.io/docs/

[Service]
ExecStart=/usr/bin/kubelet
Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target

/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
...
...
